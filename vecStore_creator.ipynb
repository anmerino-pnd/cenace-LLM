{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll make use of the Retrieval Chain to retrieve the best response from the model and give context to the response\n",
    "\n",
    "# We'll read the pdf files and convert them to text\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf1 = PyPDFLoader('ocotillo.pdf')\n",
    "page1 = pdf1.load_and_split()\n",
    "\n",
    "pdf2 = PyPDFLoader(\"MIchael-Newton-Journey-of-Souls.pdf\")\n",
    "page2 = pdf2.load_and_split()\n",
    "#pages = page1 + page2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Probar diferentes:\n",
    "    - text splitters\n",
    "    - bases de datos (FAISS, chromedb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This import is used to create a document from the text and embed it\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "# This import is used to split the text into documents\n",
    "from langchain.text_splitter import CharacterTextSplitter, SpacyTextSplitter, NLTKTextSplitter \n",
    "\n",
    "# We'll use the OllamaEmbeddings class to embed the text\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# Create an instance of the OllamaEmbeddings class\n",
    "# modelEmb = 'mxbai-embed-large:latest'\n",
    "modelEmb = 'nomic-embed-text:latest'\n",
    "embeddings = OllamaEmbeddings(model=modelEmb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the RecursiveCharacterTextSplitter class\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "# Split the text into documents\n",
    "document1 = text_splitter.split_documents(page1)\n",
    "document2 = text_splitter.split_documents(page2)\n",
    "\n",
    "#documents = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector1 = FAISS.from_documents(document1, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector2 = FAISS.from_documents(document1, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector1.docstore.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_dict': {'409c0c1a-801d-44ee-a2ff-c31f5c586755': Document(page_content='Notas Ocotillo y Ollama\\nApril 16, 2024\\nContents\\n1 Ejecutar el servidor de Ollama en un nodo de Ocotillo 1\\n1.1 Script de SLURM para servidor Ollama . . . . . . . . . . . . 2\\n1.2 Ejecutar servidor . . . . . . . . . . . . . . . . . . . . . . . . . 2\\n1.3 Probando conexión con el servidor de Ollama . . . . . . . . . 3\\n1.4 Gestión de la cola con Slurm . . . . . . . . . . . . . . . . . . 3\\n2 Establecer un tunel entre Ocotillo y local 4\\n3 Medición de tiempos 4\\n1 Ejecutar el servidor de Ollama en un nodo de\\nOcotillo\\nConsideramos que existe el usuario eacunaen Ocotillo.\\nssh eacuna@148.225.111.150\\nPuedes establecer la siguiente relación en /etc/hosts de tu local:\\n148.225.111.150 ocotillo.acarus\\nEl comando queda entonces como:\\nssh eacuna@ocotillo.acarus\\nPara ejecutar Ollama en un nodo del cluster, debemos utilizar un script\\nde Slurm, el manejador de colas que utiliza ACARUS.\\n1', metadata={'source': 'ocotillo.pdf', 'page': 0}),\n",
       "  '1777b387-7f22-44a8-ae5e-35590299d34c': Document(page_content='1.1 Script de SLURM para servidor Ollama\\nConsideramos el siguiente script de Bash llamado ollama-serve.slrm .\\n#!/bin/bash\\n#SBATCH --nodes=1\\n#SBATCH --job-name=ollama-serve\\n#SBATCH --ntasks=40\\n#SBATCH --time=24:00:00\\n#SBATCH --partition=general\\n#SBATCH --constraint=broadwell\\ncluster=$(hostname -f)\\necho -e \"\\nDirectorio compartido de modelos = /LUSTRE/home/mlg/ollama/models\\nPara conectarse a esta instancia del servidor Ollama se debe invocar\\ncon la variable de entorno\\nOLLAMA_HOST=${cluster}:11434\\nPor ejemplo, para correr el chat con el modelo Llama 2 con 7B\\nparámetros debemos ejecutar:\\n$ OLLAMA_HOST=${cluster}:11434 ollama run llama2:7b\\n\"\\nOLLAMA_DEBUG=1 \\\\\\nOLLAMA_MODELS=/LUSTRE/home/mlg/ollama/models \\\\\\nOLLAMA_HOST=0.0.0.0 \\\\\\nollama serve\\n1.2 Ejecutar servidor\\nDesde tu cuenta de Ocotillo, ejecutar el script anterior:\\nsbatch ollama-serve.slrm\\nSe mostrará un mensaje diciendo algo como:\\nSubmitted batch job 58630\\n2', metadata={'source': 'ocotillo.pdf', 'page': 1}),\n",
       "  'fd94b5e0-eca6-4e52-9a69-742258e1927b': Document(page_content='El número al final del mensaje es el identificador del proceso de Ollama.\\nUn archivo llamado slurm-<id>.out será creado, en este ejemplo es\\nslurm-58630.out .\\nAl inicio del contenido de este archivo se encuentra la variable de entorno\\nque debemos usar para conectarnos al servidor de OLLAMA, por ejemplo,\\nusando el comando:\\nhead slurm-58630.out\\nobtenemos:\\nDirectorio compartido de modelos = /LUSTRE/home/mlg/ollama/models\\nPara conectarse a esta instancia del servidor Ollama se debe invocar\\ncon la variable de entorno\\nOLLAMA_HOST=nodo18.ocotillo.unison.mx:11434\\n1.3 Probando conexión con el servidor de Ollama\\nPara probar que estos pasos han funcionado, ejecutamos una inferencia sim-\\nple sobre el modelo gemma:2b , especificando a ollamael valor apropiado de\\nOLLAMA_HOST .\\nOLLAMA_HOST=nodo18.ocotillo.unison.mx:11434 \\\\\\nollama run \\\\\\ngemma:2b \\\\\\n\"Escribe los primeros 5 números primos\" \\\\\\n--verbose\\n1.4 Gestión de la cola con Slurm\\nPuedes determinar qué tareas hay en la cola con el comando squeue. Si\\nquieres filtrar únicamente las que ha enviado un usuario puedes usar squeue\\n-u <usuario> .\\nUna vez que terminemos de utilizar el servidor de Ollama, debemos can-\\ncelar la tarea de la cola utilizando el identificador de tarea:\\nscancel 58630\\n3', metadata={'source': 'ocotillo.pdf', 'page': 2}),\n",
       "  '837d9a7c-1de5-460e-ad28-23bf3e568427': Document(page_content='2 Establecer un tunel entre Ocotillo y local\\nAl ejecutar el servidor de Ollama en Ocotillo e identificar la dirección del\\nnodo, podemos salir de la conexión SSH y abrir una nueva con un tunel que\\nredireccione las peticiones locales al puerto 11434 para la dirección del nodo\\nen el mismo puerto:\\nssh -L11434:nodo18.ocotillo.unison.mx:11434 \\\\\\neacuna@ocotillo.acarus \\\\\\n-o ServerAliveInterval=60\\nDebemos evitar cerrar esta terminal mientras utilizamos Ollama local-\\nmente. Al acceder a localhost:11434 utilizaremos el servidor de Ollama\\ncorriendo en Ocotillo.\\n3 Medición de tiempos\\nComando:\\nollama run mixtral \"Escribe los primeros 100 números primos\" --nowordwrap --verbose\\nmáquina modelo total duration load duration prompt count prompt duration prompt rate eval count eval duration eval rate\\nfuriosa mixtral 2m25.584702349s 16.105427486s 24 token(s) 912.969ms 26.29 tokens/s 947 token(s) 2m8.565994s 7.37 tokens/s\\nocotillo mixtral 3m26.917136239s 54.706411014s 24 token(s) 3.328061s 7.21 tokens/s 977 token(s) 2m28.874919s 6.56 tokens/s\\nfuriosa mistral:7b 45.295963367s 874.828221ms 23 token(s) 345.833ms 66.51 tokens/s 589 token(s) 44.074989s 13.36 tokens/s\\nocotillo mistral:7b 32.959572945s 2.089187529s 23 token(s) 866.565ms 26.54 tokens/s 350 token(s) 29.997524s 11.67 tokens/s\\nfuriosa gemma:2b 4.704500452s 860.642973ms 20 token(s) 210.688ms 94.93 tokens/s 112 token(s) 3.632854s 30.83 tokens/s\\nocotillo gemma:2b 8.961845446s 4.355539941s 20 token(s) 289.785ms 69.02 tokens/s 110 token(s) 4.313758s 25.50 tokens/s\\nfuriosa gemma:7b 1m20.937150876s 1.154339854s 20 token(s) 378.748ms 52.81 tokens/s 815 token(s) 1m19.403794s 10.26 tokens/s\\nocotillo gemma:7b 1m45.699217015s 8.135698374s 20 token(s) 928.112ms 21.55 tokens/s 815 token(s) 1m36.628237s 8.43 tokens/s\\n4', metadata={'source': 'ocotillo.pdf', 'page': 3})}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector2.docstore.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Wrong number or type of arguments for overloaded function 'IndexFlatCodes_merge_from'.\n  Possible C/C++ prototypes are:\n    faiss::IndexFlatCodes::merge_from(faiss::Index &,faiss::idx_t)\n    faiss::IndexFlatCodes::merge_from(faiss::Index &)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12068\\3341927923.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvector1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\langchain_community\\vectorstores\\faiss.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, target)\u001b[0m\n\u001b[0;32m    887\u001b[0m         \u001b[1;31m# Numerical index for target docs are incremental on existing ones\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[0mstarting_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_to_docstore_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m         \u001b[1;31m# Merge two IndexFlatL2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m         \u001b[1;31m# Get id and docs from target FAISS object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m         \u001b[0mfull_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\faiss\\swigfaiss.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, otherIndex, add_id)\u001b[0m\n\u001b[0;32m   1960\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmerge_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0motherIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1961\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_swigfaiss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIndexFlatCodes_merge_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0motherIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: Wrong number or type of arguments for overloaded function 'IndexFlatCodes_merge_from'.\n  Possible C/C++ prototypes are:\n    faiss::IndexFlatCodes::merge_from(faiss::Index &,faiss::idx_t)\n    faiss::IndexFlatCodes::merge_from(faiss::Index &)\n"
     ]
    }
   ],
   "source": [
    "vector1.merge_from()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_dict': {'b3d37175-3450-47ce-8165-f0c74f9ff1a3': Document(page_content='Notas Ocotillo y Ollama\\nApril 16, 2024\\nContents\\n1 Ejecutar el servidor de Ollama en un nodo de Ocotillo 1\\n1.1 Script de SLURM para servidor Ollama . . . . . . . . . . . . 2\\n1.2 Ejecutar servidor . . . . . . . . . . . . . . . . . . . . . . . . . 2\\n1.3 Probando conexión con el servidor de Ollama . . . . . . . . . 3\\n1.4 Gestión de la cola con Slurm . . . . . . . . . . . . . . . . . . 3\\n2 Establecer un tunel entre Ocotillo y local 4\\n3 Medición de tiempos 4\\n1 Ejecutar el servidor de Ollama en un nodo de\\nOcotillo\\nConsideramos que existe el usuario eacunaen Ocotillo.\\nssh eacuna@148.225.111.150\\nPuedes establecer la siguiente relación en /etc/hosts de tu local:\\n148.225.111.150 ocotillo.acarus\\nEl comando queda entonces como:\\nssh eacuna@ocotillo.acarus\\nPara ejecutar Ollama en un nodo del cluster, debemos utilizar un script\\nde Slurm, el manejador de colas que utiliza ACARUS.\\n1', metadata={'source': 'ocotillo.pdf', 'page': 0}),\n",
       "  '75664ff1-5075-4f3f-bb2f-a42b1057a3dc': Document(page_content='1.1 Script de SLURM para servidor Ollama\\nConsideramos el siguiente script de Bash llamado ollama-serve.slrm .\\n#!/bin/bash\\n#SBATCH --nodes=1\\n#SBATCH --job-name=ollama-serve\\n#SBATCH --ntasks=40\\n#SBATCH --time=24:00:00\\n#SBATCH --partition=general\\n#SBATCH --constraint=broadwell\\ncluster=$(hostname -f)\\necho -e \"\\nDirectorio compartido de modelos = /LUSTRE/home/mlg/ollama/models\\nPara conectarse a esta instancia del servidor Ollama se debe invocar\\ncon la variable de entorno\\nOLLAMA_HOST=${cluster}:11434\\nPor ejemplo, para correr el chat con el modelo Llama 2 con 7B\\nparámetros debemos ejecutar:\\n$ OLLAMA_HOST=${cluster}:11434 ollama run llama2:7b\\n\"\\nOLLAMA_DEBUG=1 \\\\\\nOLLAMA_MODELS=/LUSTRE/home/mlg/ollama/models \\\\\\nOLLAMA_HOST=0.0.0.0 \\\\\\nollama serve\\n1.2 Ejecutar servidor\\nDesde tu cuenta de Ocotillo, ejecutar el script anterior:\\nsbatch ollama-serve.slrm\\nSe mostrará un mensaje diciendo algo como:\\nSubmitted batch job 58630\\n2', metadata={'source': 'ocotillo.pdf', 'page': 1}),\n",
       "  '56d2d801-a986-4da2-babe-5e8dbf780987': Document(page_content='El número al final del mensaje es el identificador del proceso de Ollama.\\nUn archivo llamado slurm-<id>.out será creado, en este ejemplo es\\nslurm-58630.out .\\nAl inicio del contenido de este archivo se encuentra la variable de entorno\\nque debemos usar para conectarnos al servidor de OLLAMA, por ejemplo,\\nusando el comando:\\nhead slurm-58630.out\\nobtenemos:\\nDirectorio compartido de modelos = /LUSTRE/home/mlg/ollama/models\\nPara conectarse a esta instancia del servidor Ollama se debe invocar\\ncon la variable de entorno\\nOLLAMA_HOST=nodo18.ocotillo.unison.mx:11434\\n1.3 Probando conexión con el servidor de Ollama\\nPara probar que estos pasos han funcionado, ejecutamos una inferencia sim-\\nple sobre el modelo gemma:2b , especificando a ollamael valor apropiado de\\nOLLAMA_HOST .\\nOLLAMA_HOST=nodo18.ocotillo.unison.mx:11434 \\\\\\nollama run \\\\\\ngemma:2b \\\\\\n\"Escribe los primeros 5 números primos\" \\\\\\n--verbose\\n1.4 Gestión de la cola con Slurm\\nPuedes determinar qué tareas hay en la cola con el comando squeue. Si\\nquieres filtrar únicamente las que ha enviado un usuario puedes usar squeue\\n-u <usuario> .\\nUna vez que terminemos de utilizar el servidor de Ollama, debemos can-\\ncelar la tarea de la cola utilizando el identificador de tarea:\\nscancel 58630\\n3', metadata={'source': 'ocotillo.pdf', 'page': 2}),\n",
       "  'ce9557fb-ee94-4faa-8d7b-681a6b6e9eb5': Document(page_content='2 Establecer un tunel entre Ocotillo y local\\nAl ejecutar el servidor de Ollama en Ocotillo e identificar la dirección del\\nnodo, podemos salir de la conexión SSH y abrir una nueva con un tunel que\\nredireccione las peticiones locales al puerto 11434 para la dirección del nodo\\nen el mismo puerto:\\nssh -L11434:nodo18.ocotillo.unison.mx:11434 \\\\\\neacuna@ocotillo.acarus \\\\\\n-o ServerAliveInterval=60\\nDebemos evitar cerrar esta terminal mientras utilizamos Ollama local-\\nmente. Al acceder a localhost:11434 utilizaremos el servidor de Ollama\\ncorriendo en Ocotillo.\\n3 Medición de tiempos\\nComando:\\nollama run mixtral \"Escribe los primeros 100 números primos\" --nowordwrap --verbose\\nmáquina modelo total duration load duration prompt count prompt duration prompt rate eval count eval duration eval rate\\nfuriosa mixtral 2m25.584702349s 16.105427486s 24 token(s) 912.969ms 26.29 tokens/s 947 token(s) 2m8.565994s 7.37 tokens/s\\nocotillo mixtral 3m26.917136239s 54.706411014s 24 token(s) 3.328061s 7.21 tokens/s 977 token(s) 2m28.874919s 6.56 tokens/s\\nfuriosa mistral:7b 45.295963367s 874.828221ms 23 token(s) 345.833ms 66.51 tokens/s 589 token(s) 44.074989s 13.36 tokens/s\\nocotillo mistral:7b 32.959572945s 2.089187529s 23 token(s) 866.565ms 26.54 tokens/s 350 token(s) 29.997524s 11.67 tokens/s\\nfuriosa gemma:2b 4.704500452s 860.642973ms 20 token(s) 210.688ms 94.93 tokens/s 112 token(s) 3.632854s 30.83 tokens/s\\nocotillo gemma:2b 8.961845446s 4.355539941s 20 token(s) 289.785ms 69.02 tokens/s 110 token(s) 4.313758s 25.50 tokens/s\\nfuriosa gemma:7b 1m20.937150876s 1.154339854s 20 token(s) 378.748ms 52.81 tokens/s 815 token(s) 1m19.403794s 10.26 tokens/s\\nocotillo gemma:7b 1m45.699217015s 8.135698374s 20 token(s) 928.112ms 21.55 tokens/s 815 token(s) 1m36.628237s 8.43 tokens/s\\n4', metadata={'source': 'ocotillo.pdf', 'page': 3}),\n",
       "  '409c0c1a-801d-44ee-a2ff-c31f5c586755': Document(page_content='Notas Ocotillo y Ollama\\nApril 16, 2024\\nContents\\n1 Ejecutar el servidor de Ollama en un nodo de Ocotillo 1\\n1.1 Script de SLURM para servidor Ollama . . . . . . . . . . . . 2\\n1.2 Ejecutar servidor . . . . . . . . . . . . . . . . . . . . . . . . . 2\\n1.3 Probando conexión con el servidor de Ollama . . . . . . . . . 3\\n1.4 Gestión de la cola con Slurm . . . . . . . . . . . . . . . . . . 3\\n2 Establecer un tunel entre Ocotillo y local 4\\n3 Medición de tiempos 4\\n1 Ejecutar el servidor de Ollama en un nodo de\\nOcotillo\\nConsideramos que existe el usuario eacunaen Ocotillo.\\nssh eacuna@148.225.111.150\\nPuedes establecer la siguiente relación en /etc/hosts de tu local:\\n148.225.111.150 ocotillo.acarus\\nEl comando queda entonces como:\\nssh eacuna@ocotillo.acarus\\nPara ejecutar Ollama en un nodo del cluster, debemos utilizar un script\\nde Slurm, el manejador de colas que utiliza ACARUS.\\n1', metadata={'source': 'ocotillo.pdf', 'page': 0}),\n",
       "  '1777b387-7f22-44a8-ae5e-35590299d34c': Document(page_content='1.1 Script de SLURM para servidor Ollama\\nConsideramos el siguiente script de Bash llamado ollama-serve.slrm .\\n#!/bin/bash\\n#SBATCH --nodes=1\\n#SBATCH --job-name=ollama-serve\\n#SBATCH --ntasks=40\\n#SBATCH --time=24:00:00\\n#SBATCH --partition=general\\n#SBATCH --constraint=broadwell\\ncluster=$(hostname -f)\\necho -e \"\\nDirectorio compartido de modelos = /LUSTRE/home/mlg/ollama/models\\nPara conectarse a esta instancia del servidor Ollama se debe invocar\\ncon la variable de entorno\\nOLLAMA_HOST=${cluster}:11434\\nPor ejemplo, para correr el chat con el modelo Llama 2 con 7B\\nparámetros debemos ejecutar:\\n$ OLLAMA_HOST=${cluster}:11434 ollama run llama2:7b\\n\"\\nOLLAMA_DEBUG=1 \\\\\\nOLLAMA_MODELS=/LUSTRE/home/mlg/ollama/models \\\\\\nOLLAMA_HOST=0.0.0.0 \\\\\\nollama serve\\n1.2 Ejecutar servidor\\nDesde tu cuenta de Ocotillo, ejecutar el script anterior:\\nsbatch ollama-serve.slrm\\nSe mostrará un mensaje diciendo algo como:\\nSubmitted batch job 58630\\n2', metadata={'source': 'ocotillo.pdf', 'page': 1}),\n",
       "  'fd94b5e0-eca6-4e52-9a69-742258e1927b': Document(page_content='El número al final del mensaje es el identificador del proceso de Ollama.\\nUn archivo llamado slurm-<id>.out será creado, en este ejemplo es\\nslurm-58630.out .\\nAl inicio del contenido de este archivo se encuentra la variable de entorno\\nque debemos usar para conectarnos al servidor de OLLAMA, por ejemplo,\\nusando el comando:\\nhead slurm-58630.out\\nobtenemos:\\nDirectorio compartido de modelos = /LUSTRE/home/mlg/ollama/models\\nPara conectarse a esta instancia del servidor Ollama se debe invocar\\ncon la variable de entorno\\nOLLAMA_HOST=nodo18.ocotillo.unison.mx:11434\\n1.3 Probando conexión con el servidor de Ollama\\nPara probar que estos pasos han funcionado, ejecutamos una inferencia sim-\\nple sobre el modelo gemma:2b , especificando a ollamael valor apropiado de\\nOLLAMA_HOST .\\nOLLAMA_HOST=nodo18.ocotillo.unison.mx:11434 \\\\\\nollama run \\\\\\ngemma:2b \\\\\\n\"Escribe los primeros 5 números primos\" \\\\\\n--verbose\\n1.4 Gestión de la cola con Slurm\\nPuedes determinar qué tareas hay en la cola con el comando squeue. Si\\nquieres filtrar únicamente las que ha enviado un usuario puedes usar squeue\\n-u <usuario> .\\nUna vez que terminemos de utilizar el servidor de Ollama, debemos can-\\ncelar la tarea de la cola utilizando el identificador de tarea:\\nscancel 58630\\n3', metadata={'source': 'ocotillo.pdf', 'page': 2}),\n",
       "  '837d9a7c-1de5-460e-ad28-23bf3e568427': Document(page_content='2 Establecer un tunel entre Ocotillo y local\\nAl ejecutar el servidor de Ollama en Ocotillo e identificar la dirección del\\nnodo, podemos salir de la conexión SSH y abrir una nueva con un tunel que\\nredireccione las peticiones locales al puerto 11434 para la dirección del nodo\\nen el mismo puerto:\\nssh -L11434:nodo18.ocotillo.unison.mx:11434 \\\\\\neacuna@ocotillo.acarus \\\\\\n-o ServerAliveInterval=60\\nDebemos evitar cerrar esta terminal mientras utilizamos Ollama local-\\nmente. Al acceder a localhost:11434 utilizaremos el servidor de Ollama\\ncorriendo en Ocotillo.\\n3 Medición de tiempos\\nComando:\\nollama run mixtral \"Escribe los primeros 100 números primos\" --nowordwrap --verbose\\nmáquina modelo total duration load duration prompt count prompt duration prompt rate eval count eval duration eval rate\\nfuriosa mixtral 2m25.584702349s 16.105427486s 24 token(s) 912.969ms 26.29 tokens/s 947 token(s) 2m8.565994s 7.37 tokens/s\\nocotillo mixtral 3m26.917136239s 54.706411014s 24 token(s) 3.328061s 7.21 tokens/s 977 token(s) 2m28.874919s 6.56 tokens/s\\nfuriosa mistral:7b 45.295963367s 874.828221ms 23 token(s) 345.833ms 66.51 tokens/s 589 token(s) 44.074989s 13.36 tokens/s\\nocotillo mistral:7b 32.959572945s 2.089187529s 23 token(s) 866.565ms 26.54 tokens/s 350 token(s) 29.997524s 11.67 tokens/s\\nfuriosa gemma:2b 4.704500452s 860.642973ms 20 token(s) 210.688ms 94.93 tokens/s 112 token(s) 3.632854s 30.83 tokens/s\\nocotillo gemma:2b 8.961845446s 4.355539941s 20 token(s) 289.785ms 69.02 tokens/s 110 token(s) 4.313758s 25.50 tokens/s\\nfuriosa gemma:7b 1m20.937150876s 1.154339854s 20 token(s) 378.748ms 52.81 tokens/s 815 token(s) 1m19.403794s 10.26 tokens/s\\nocotillo gemma:7b 1m45.699217015s 8.135698374s 20 token(s) 928.112ms 21.55 tokens/s 815 token(s) 1m36.628237s 8.43 tokens/s\\n4', metadata={'source': 'ocotillo.pdf', 'page': 3})}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector1.docstore.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FAISS' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m       existing_vector_store\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mid\u001b[39m, data)  \u001b[38;5;66;03m# Add if not found\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Usage\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mupdate_vector_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector2\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m, in \u001b[0;36mupdate_vector_store\u001b[1;34m(existing_vector_store, new_vector_store)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_vector_store\u001b[39m(existing_vector_store, new_vector_store):\n\u001b[1;32m----> 2\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m, data \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnew_vector_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m():\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m existing_vector_store\u001b[38;5;241m.\u001b[39msearch(data, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Check if ID exists\u001b[39;00m\n\u001b[0;32m      4\u001b[0m       \u001b[38;5;66;03m# Update logic here (assuming existing_vector_store has an update method)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m       \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'FAISS' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "def update_vector_store(existing_vector_store, new_vector_store):\n",
    "  for id, data in new_vector_store.items():\n",
    "    if existing_vector_store.search(data, 1)[1][0] == 0:  # Check if ID exists\n",
    "      # Update logic here (assuming existing_vector_store has an update method)\n",
    "      pass\n",
    "    else:\n",
    "      existing_vector_store.add(id, data)  # Add if not found\n",
    "\n",
    "# Usage\n",
    "update_vector_store(vector1, vector2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'89f39d05-25ee-477c-b0ca-56975753abcd': Document(page_content='Notas Ocotillo y Ollama\\nApril 16, 2024\\nContents\\n1 Ejecutar el servidor de Ollama en un nodo de Ocotillo 1\\n1.1 Script de SLURM para servidor Ollama . . . . . . . . . . . . 2\\n1.2 Ejecutar servidor . . . . . . . . . . . . . . . . . . . . . . . . . 2\\n1.3 Probando conexión con el servidor de Ollama . . . . . . . . . 3\\n1.4 Gestión de la cola con Slurm . . . . . . . . . . . . . . . . . . 3\\n2 Establecer un tunel entre Ocotillo y local 4\\n3 Medición de tiempos 4\\n1 Ejecutar el servidor de Ollama en un nodo de\\nOcotillo\\nConsideramos que existe el usuario eacunaen Ocotillo.\\nssh eacuna@148.225.111.150\\nPuedes establecer la siguiente relación en /etc/hosts de tu local:\\n148.225.111.150 ocotillo.acarus\\nEl comando queda entonces como:\\nssh eacuna@ocotillo.acarus\\nPara ejecutar Ollama en un nodo del cluster, debemos utilizar un script\\nde Slurm, el manejador de colas que utiliza ACARUS.\\n1', metadata={'source': 'ocotillo.pdf', 'page': 0}),\n",
       " 'e23ab021-4cf3-4e69-9222-c545074f7d9a': Document(page_content='1.1 Script de SLURM para servidor Ollama\\nConsideramos el siguiente script de Bash llamado ollama-serve.slrm .\\n#!/bin/bash\\n#SBATCH --nodes=1\\n#SBATCH --job-name=ollama-serve\\n#SBATCH --ntasks=40\\n#SBATCH --time=24:00:00\\n#SBATCH --partition=general\\n#SBATCH --constraint=broadwell\\ncluster=$(hostname -f)\\necho -e \"\\nDirectorio compartido de modelos = /LUSTRE/home/mlg/ollama/models\\nPara conectarse a esta instancia del servidor Ollama se debe invocar\\ncon la variable de entorno\\nOLLAMA_HOST=${cluster}:11434\\nPor ejemplo, para correr el chat con el modelo Llama 2 con 7B\\nparámetros debemos ejecutar:\\n$ OLLAMA_HOST=${cluster}:11434 ollama run llama2:7b\\n\"\\nOLLAMA_DEBUG=1 \\\\\\nOLLAMA_MODELS=/LUSTRE/home/mlg/ollama/models \\\\\\nOLLAMA_HOST=0.0.0.0 \\\\\\nollama serve\\n1.2 Ejecutar servidor\\nDesde tu cuenta de Ocotillo, ejecutar el script anterior:\\nsbatch ollama-serve.slrm\\nSe mostrará un mensaje diciendo algo como:\\nSubmitted batch job 58630\\n2', metadata={'source': 'ocotillo.pdf', 'page': 1}),\n",
       " '8c9877a9-b2f4-4f6b-ae5b-cf892071f669': Document(page_content='El número al final del mensaje es el identificador del proceso de Ollama.\\nUn archivo llamado slurm-<id>.out será creado, en este ejemplo es\\nslurm-58630.out .\\nAl inicio del contenido de este archivo se encuentra la variable de entorno\\nque debemos usar para conectarnos al servidor de OLLAMA, por ejemplo,\\nusando el comando:\\nhead slurm-58630.out\\nobtenemos:\\nDirectorio compartido de modelos = /LUSTRE/home/mlg/ollama/models\\nPara conectarse a esta instancia del servidor Ollama se debe invocar\\ncon la variable de entorno\\nOLLAMA_HOST=nodo18.ocotillo.unison.mx:11434\\n1.3 Probando conexión con el servidor de Ollama\\nPara probar que estos pasos han funcionado, ejecutamos una inferencia sim-\\nple sobre el modelo gemma:2b , especificando a ollamael valor apropiado de\\nOLLAMA_HOST .\\nOLLAMA_HOST=nodo18.ocotillo.unison.mx:11434 \\\\\\nollama run \\\\\\ngemma:2b \\\\\\n\"Escribe los primeros 5 números primos\" \\\\\\n--verbose\\n1.4 Gestión de la cola con Slurm\\nPuedes determinar qué tareas hay en la cola con el comando squeue. Si\\nquieres filtrar únicamente las que ha enviado un usuario puedes usar squeue\\n-u <usuario> .\\nUna vez que terminemos de utilizar el servidor de Ollama, debemos can-\\ncelar la tarea de la cola utilizando el identificador de tarea:\\nscancel 58630\\n3', metadata={'source': 'ocotillo.pdf', 'page': 2}),\n",
       " '7498a5d1-487b-496b-9a49-4f5c66c50c2c': Document(page_content='2 Establecer un tunel entre Ocotillo y local\\nAl ejecutar el servidor de Ollama en Ocotillo e identificar la dirección del\\nnodo, podemos salir de la conexión SSH y abrir una nueva con un tunel que\\nredireccione las peticiones locales al puerto 11434 para la dirección del nodo\\nen el mismo puerto:\\nssh -L11434:nodo18.ocotillo.unison.mx:11434 \\\\\\neacuna@ocotillo.acarus \\\\\\n-o ServerAliveInterval=60\\nDebemos evitar cerrar esta terminal mientras utilizamos Ollama local-\\nmente. Al acceder a localhost:11434 utilizaremos el servidor de Ollama\\ncorriendo en Ocotillo.\\n3 Medición de tiempos\\nComando:\\nollama run mixtral \"Escribe los primeros 100 números primos\" --nowordwrap --verbose\\nmáquina modelo total duration load duration prompt count prompt duration prompt rate eval count eval duration eval rate\\nfuriosa mixtral 2m25.584702349s 16.105427486s 24 token(s) 912.969ms 26.29 tokens/s 947 token(s) 2m8.565994s 7.37 tokens/s\\nocotillo mixtral 3m26.917136239s 54.706411014s 24 token(s) 3.328061s 7.21 tokens/s 977 token(s) 2m28.874919s 6.56 tokens/s\\nfuriosa mistral:7b 45.295963367s 874.828221ms 23 token(s) 345.833ms 66.51 tokens/s 589 token(s) 44.074989s 13.36 tokens/s\\nocotillo mistral:7b 32.959572945s 2.089187529s 23 token(s) 866.565ms 26.54 tokens/s 350 token(s) 29.997524s 11.67 tokens/s\\nfuriosa gemma:2b 4.704500452s 860.642973ms 20 token(s) 210.688ms 94.93 tokens/s 112 token(s) 3.632854s 30.83 tokens/s\\nocotillo gemma:2b 8.961845446s 4.355539941s 20 token(s) 289.785ms 69.02 tokens/s 110 token(s) 4.313758s 25.50 tokens/s\\nfuriosa gemma:7b 1m20.937150876s 1.154339854s 20 token(s) 378.748ms 52.81 tokens/s 815 token(s) 1m19.403794s 10.26 tokens/s\\nocotillo gemma:7b 1m45.699217015s 8.135698374s 20 token(s) 928.112ms 21.55 tokens/s 815 token(s) 1m36.628237s 8.43 tokens/s\\n4', metadata={'source': 'ocotillo.pdf', 'page': 3}),\n",
       " '0b20d4fa-4b5c-4319-ba0f-7d69b6c19357': Document(page_content='Notas Ocotillo y Ollama\\nApril 16, 2024\\nContents\\n1 Ejecutar el servidor de Ollama en un nodo de Ocotillo 1\\n1.1 Script de SLURM para servidor Ollama . . . . . . . . . . . . 2\\n1.2 Ejecutar servidor . . . . . . . . . . . . . . . . . . . . . . . . . 2\\n1.3 Probando conexión con el servidor de Ollama . . . . . . . . . 3\\n1.4 Gestión de la cola con Slurm . . . . . . . . . . . . . . . . . . 3\\n2 Establecer un tunel entre Ocotillo y local 4\\n3 Medición de tiempos 4\\n1 Ejecutar el servidor de Ollama en un nodo de\\nOcotillo\\nConsideramos que existe el usuario eacunaen Ocotillo.\\nssh eacuna@148.225.111.150\\nPuedes establecer la siguiente relación en /etc/hosts de tu local:\\n148.225.111.150 ocotillo.acarus\\nEl comando queda entonces como:\\nssh eacuna@ocotillo.acarus\\nPara ejecutar Ollama en un nodo del cluster, debemos utilizar un script\\nde Slurm, el manejador de colas que utiliza ACARUS.\\n1', metadata={'source': 'ocotillo.pdf', 'page': 0}),\n",
       " '537a0b9c-d9de-47e4-81fd-142af1d0480a': Document(page_content='1.1 Script de SLURM para servidor Ollama\\nConsideramos el siguiente script de Bash llamado ollama-serve.slrm .\\n#!/bin/bash\\n#SBATCH --nodes=1\\n#SBATCH --job-name=ollama-serve\\n#SBATCH --ntasks=40\\n#SBATCH --time=24:00:00\\n#SBATCH --partition=general\\n#SBATCH --constraint=broadwell\\ncluster=$(hostname -f)\\necho -e \"\\nDirectorio compartido de modelos = /LUSTRE/home/mlg/ollama/models\\nPara conectarse a esta instancia del servidor Ollama se debe invocar\\ncon la variable de entorno\\nOLLAMA_HOST=${cluster}:11434\\nPor ejemplo, para correr el chat con el modelo Llama 2 con 7B\\nparámetros debemos ejecutar:\\n$ OLLAMA_HOST=${cluster}:11434 ollama run llama2:7b\\n\"\\nOLLAMA_DEBUG=1 \\\\\\nOLLAMA_MODELS=/LUSTRE/home/mlg/ollama/models \\\\\\nOLLAMA_HOST=0.0.0.0 \\\\\\nollama serve\\n1.2 Ejecutar servidor\\nDesde tu cuenta de Ocotillo, ejecutar el script anterior:\\nsbatch ollama-serve.slrm\\nSe mostrará un mensaje diciendo algo como:\\nSubmitted batch job 58630\\n2', metadata={'source': 'ocotillo.pdf', 'page': 1}),\n",
       " 'cdc38ae1-1e61-40e6-9b1f-b80fed644bf4': Document(page_content='El número al final del mensaje es el identificador del proceso de Ollama.\\nUn archivo llamado slurm-<id>.out será creado, en este ejemplo es\\nslurm-58630.out .\\nAl inicio del contenido de este archivo se encuentra la variable de entorno\\nque debemos usar para conectarnos al servidor de OLLAMA, por ejemplo,\\nusando el comando:\\nhead slurm-58630.out\\nobtenemos:\\nDirectorio compartido de modelos = /LUSTRE/home/mlg/ollama/models\\nPara conectarse a esta instancia del servidor Ollama se debe invocar\\ncon la variable de entorno\\nOLLAMA_HOST=nodo18.ocotillo.unison.mx:11434\\n1.3 Probando conexión con el servidor de Ollama\\nPara probar que estos pasos han funcionado, ejecutamos una inferencia sim-\\nple sobre el modelo gemma:2b , especificando a ollamael valor apropiado de\\nOLLAMA_HOST .\\nOLLAMA_HOST=nodo18.ocotillo.unison.mx:11434 \\\\\\nollama run \\\\\\ngemma:2b \\\\\\n\"Escribe los primeros 5 números primos\" \\\\\\n--verbose\\n1.4 Gestión de la cola con Slurm\\nPuedes determinar qué tareas hay en la cola con el comando squeue. Si\\nquieres filtrar únicamente las que ha enviado un usuario puedes usar squeue\\n-u <usuario> .\\nUna vez que terminemos de utilizar el servidor de Ollama, debemos can-\\ncelar la tarea de la cola utilizando el identificador de tarea:\\nscancel 58630\\n3', metadata={'source': 'ocotillo.pdf', 'page': 2}),\n",
       " '3b933480-9d9c-48b5-9640-774bfdd0afa2': Document(page_content='2 Establecer un tunel entre Ocotillo y local\\nAl ejecutar el servidor de Ollama en Ocotillo e identificar la dirección del\\nnodo, podemos salir de la conexión SSH y abrir una nueva con un tunel que\\nredireccione las peticiones locales al puerto 11434 para la dirección del nodo\\nen el mismo puerto:\\nssh -L11434:nodo18.ocotillo.unison.mx:11434 \\\\\\neacuna@ocotillo.acarus \\\\\\n-o ServerAliveInterval=60\\nDebemos evitar cerrar esta terminal mientras utilizamos Ollama local-\\nmente. Al acceder a localhost:11434 utilizaremos el servidor de Ollama\\ncorriendo en Ocotillo.\\n3 Medición de tiempos\\nComando:\\nollama run mixtral \"Escribe los primeros 100 números primos\" --nowordwrap --verbose\\nmáquina modelo total duration load duration prompt count prompt duration prompt rate eval count eval duration eval rate\\nfuriosa mixtral 2m25.584702349s 16.105427486s 24 token(s) 912.969ms 26.29 tokens/s 947 token(s) 2m8.565994s 7.37 tokens/s\\nocotillo mixtral 3m26.917136239s 54.706411014s 24 token(s) 3.328061s 7.21 tokens/s 977 token(s) 2m28.874919s 6.56 tokens/s\\nfuriosa mistral:7b 45.295963367s 874.828221ms 23 token(s) 345.833ms 66.51 tokens/s 589 token(s) 44.074989s 13.36 tokens/s\\nocotillo mistral:7b 32.959572945s 2.089187529s 23 token(s) 866.565ms 26.54 tokens/s 350 token(s) 29.997524s 11.67 tokens/s\\nfuriosa gemma:2b 4.704500452s 860.642973ms 20 token(s) 210.688ms 94.93 tokens/s 112 token(s) 3.632854s 30.83 tokens/s\\nocotillo gemma:2b 8.961845446s 4.355539941s 20 token(s) 289.785ms 69.02 tokens/s 110 token(s) 4.313758s 25.50 tokens/s\\nfuriosa gemma:7b 1m20.937150876s 1.154339854s 20 token(s) 378.748ms 52.81 tokens/s 815 token(s) 1m19.403794s 10.26 tokens/s\\nocotillo gemma:7b 1m45.699217015s 8.135698374s 20 token(s) 928.112ms 21.55 tokens/s 815 token(s) 1m36.628237s 8.43 tokens/s\\n4', metadata={'source': 'ocotillo.pdf', 'page': 3})}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector1.docstore._dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector store from the documents and embeddings\n",
    "#vector = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# Time = 33m 33.9s CharacterFAISSmxbai\n",
    "# Time = 29m 14.7s CharacterFAISSnomic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el vector store\n",
    "#vector.save_local(\"Character_FAISS_nomic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
