{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll make use of the Retrieval Chain to retrieve the best response from the model and give context to the response\n",
    "\n",
    "# We'll read the pdf files and convert them to text\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf1 = PyPDFLoader('PI_C37118_1.1.0.17.pdf')\n",
    "page1 = pdf1.load_and_split()\n",
    "\n",
    "pdf2 = PyPDFLoader(\"PI-System-Explorer-2018-User-Guide_EN.pdf\")\n",
    "page2 = pdf2.load_and_split()\n",
    "pages = page1 + page2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Probar diferentes:\n",
    "    - text splitters\n",
    "    - bases de datos (FAISS, chromedb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This import is used to create a document from the text and embed it\n",
    "from langchain_community.vectorstores import FAISS, chroma\n",
    "# This import is used to split the text into documents\n",
    "from langchain.text_splitter import CharacterTextSplitter, SpacyTextSplitter, NLTKTextSplitter \n",
    "\n",
    "# We'll use the OllamaEmbeddings class to embed the text\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# Create an instance of the OllamaEmbeddings class\n",
    "# modelEmb = 'mxbai-embed-large:latest'\n",
    "modelEmb = 'nomic-embed-text:latest'\n",
    "embeddings = OllamaEmbeddings(model=modelEmb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m \u001b[0mQdrant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_retriever\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Any'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m'VectorStoreRetriever'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Return VectorStoreRetriever initialized from this VectorStore.\n",
      "\n",
      "Args:\n",
      "    search_type (Optional[str]): Defines the type of search that\n",
      "        the Retriever should perform.\n",
      "        Can be \"similarity\" (default), \"mmr\", or\n",
      "        \"similarity_score_threshold\".\n",
      "    search_kwargs (Optional[Dict]): Keyword arguments to pass to the\n",
      "        search function. Can include things like:\n",
      "            k: Amount of documents to return (Default: 4)\n",
      "            score_threshold: Minimum relevance threshold\n",
      "                for similarity_score_threshold\n",
      "            fetch_k: Amount of documents to pass to MMR algorithm (Default: 20)\n",
      "            lambda_mult: Diversity of results returned by MMR;\n",
      "                1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
      "            filter: Filter by document metadata\n",
      "\n",
      "Returns:\n",
      "    VectorStoreRetriever: Retriever class for VectorStore.\n",
      "\n",
      "Examples:\n",
      "\n",
      ".. code-block:: python\n",
      "\n",
      "    # Retrieve more documents with higher diversity\n",
      "    # Useful if your dataset has many similar documents\n",
      "    docsearch.as_retriever(\n",
      "        search_type=\"mmr\",\n",
      "        search_kwargs={'k': 6, 'lambda_mult': 0.25}\n",
      "    )\n",
      "\n",
      "    # Fetch more documents for the MMR algorithm to consider\n",
      "    # But only return the top 5\n",
      "    docsearch.as_retriever(\n",
      "        search_type=\"mmr\",\n",
      "        search_kwargs={'k': 5, 'fetch_k': 50}\n",
      "    )\n",
      "\n",
      "    # Only retrieve documents that have a relevance score\n",
      "    # Above a certain threshold\n",
      "    docsearch.as_retriever(\n",
      "        search_type=\"similarity_score_threshold\",\n",
      "        search_kwargs={'score_threshold': 0.8}\n",
      "    )\n",
      "\n",
      "    # Only get the single most similar document from the dataset\n",
      "    docsearch.as_retriever(search_kwargs={'k': 1})\n",
      "\n",
      "    # Use a filter to only retrieve documents from a specific paper\n",
      "    docsearch.as_retriever(\n",
      "        search_kwargs={'filter': {'paper_title':'GPT-4 Technical Report'}}\n",
      "    )\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\panda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages\\langchain_core\\vectorstores.py\n",
      "\u001b[1;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "\n",
    "Qdrant.as_retriever?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m\n",
      "\u001b[0mConversationalRetrievalChain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmemory\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlangchain_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBaseMemory\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlangchain_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBaseCallbackHandler\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBaseCallbackManager\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtags\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmetadata\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcallback_manager\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlangchain_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBaseCallbackManager\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcombine_docs_chain\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchains\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcombine_documents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBaseCombineDocumentsChain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mquestion_generator\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchains\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLLMChain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0moutput_key\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'answer'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mrephrase_question\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mreturn_source_documents\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mreturn_generated_question\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mget_chat_history\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBaseMessage\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mresponse_if_no_docs_found\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mretriever\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrievers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBaseRetriever\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmax_tokens_limit\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "Chain for having a conversation based on retrieved documents.\n",
      "\n",
      "This chain takes in chat history (a list of messages) and new questions,\n",
      "and then returns an answer to that question.\n",
      "The algorithm for this chain consists of three parts:\n",
      "\n",
      "1. Use the chat history and the new question to create a \"standalone question\".\n",
      "This is done so that this question can be passed into the retrieval step to fetch\n",
      "relevant documents. If only the new question was passed in, then relevant context\n",
      "may be lacking. If the whole conversation was passed into retrieval, there may\n",
      "be unnecessary information there that would distract from retrieval.\n",
      "\n",
      "2. This new question is passed to the retriever and relevant documents are\n",
      "returned.\n",
      "\n",
      "3. The retrieved documents are passed to an LLM along with either the new question\n",
      "(default behavior) or the original question and chat history to generate a final\n",
      "response.\n",
      "\n",
      "Example:\n",
      "    .. code-block:: python\n",
      "\n",
      "        from langchain.chains import (\n",
      "            StuffDocumentsChain, LLMChain, ConversationalRetrievalChain\n",
      "        )\n",
      "        from langchain_core.prompts import PromptTemplate\n",
      "        from langchain_community.llms import OpenAI\n",
      "\n",
      "        combine_docs_chain = StuffDocumentsChain(...)\n",
      "        vectorstore = ...\n",
      "        retriever = vectorstore.as_retriever()\n",
      "\n",
      "        # This controls how the standalone question is generated.\n",
      "        # Should take `chat_history` and `question` as input variables.\n",
      "        template = (\n",
      "            \"Combine the chat history and follow up question into \"\n",
      "            \"a standalone question. Chat History: {chat_history}\"\n",
      "            \"Follow up question: {question}\"\n",
      "        )\n",
      "        prompt = PromptTemplate.from_template(template)\n",
      "        llm = OpenAI()\n",
      "        question_generator_chain = LLMChain(llm=llm, prompt=prompt)\n",
      "        chain = ConversationalRetrievalChain(\n",
      "            combine_docs_chain=combine_docs_chain,\n",
      "            retriever=retriever,\n",
      "            question_generator=question_generator_chain,\n",
      "        )\n",
      "\u001b[1;31mInit docstring:\u001b[0m\n",
      "Create a new model by parsing and validating input data from keyword arguments.\n",
      "\n",
      "Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "\u001b[1;31mFile:\u001b[0m           c:\\users\\panda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages\\langchain\\chains\\conversational_retrieval\\base.py\n",
      "\u001b[1;31mType:\u001b[0m           ModelMetaclass\n",
      "\u001b[1;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "ConversationalRetrievalChain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the RecursiveCharacterTextSplitter class\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "# Split the text into documents\n",
    "documents = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector store from the documents and embeddings\n",
    "vector = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# Time = 33m 33.9s CharacterFAISSmxbai\n",
    "# Time = 29m 14.7s CharacterFAISSnomic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el vector store\n",
    "vector.save_local(\"Character_FAISS_nomic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
